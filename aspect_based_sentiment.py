# -*- coding: utf-8 -*-
"""aspect_based_sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f16mWpdx8QJxiQNSLDpRiTnhVQYoVv1R
"""

# !pip install tensorflow==2.13.1
# !pip install gensim
# !pip install nltk
# !pip install scikit-learn
# !pip install numpy
# !pip install matplotlib
# !pip install pandas
import subprocess

def install_packages():
    packages = [
        "tensorflow==2.13.1",
        "gensim",
        "nltk",
        "scikit-learn",
        "numpy",
        "matplotlib",
        "pandas"
    ]
    
    for package in packages:
        subprocess.check_call(["pip", "install", package])

if __name__ == "__main__":
    install_packages()

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention, Flatten, Concatenate, Dropout
import numpy as np
import matplotlib.pyplot as plt
import gensim
from sklearn.feature_extraction.text import CountVectorizer
import os
import tarfile
import urllib.request
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag
import string
from collections import Counter
from tensorflow.keras import regularizers
import matplotlib.pyplot as plt
import pandas as pd
from collections import defaultdict

# Download NLTK resources

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

# Download and extract the dataset
url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'
filename = 'aclImdb_v1.tar.gz'

if not os.path.exists(filename):
    urllib.request.urlretrieve(url, filename)

if not os.path.exists('aclImdb'):
    with tarfile.open(filename, 'r:gz') as tar:
        tar.extractall()

# Load the reviews and their labels
def load_imdb_data(data_dir):
    data = []
    labels = []
    for label_type in ['pos', 'neg']:
        dir_name = os.path.join(data_dir, label_type)
        for fname in os.listdir(dir_name):
            if fname.endswith('.txt'):
                with open(os.path.join(dir_name, fname), encoding='utf-8') as f:
                    data.append(f.read())
                labels.append(1 if label_type == 'pos' else 0)
    return data, labels

train_data, train_labels = load_imdb_data('aclImdb/train')
test_data, test_labels = load_imdb_data('aclImdb/test')

# Tokenize the reviews
max_features = 10000  # Number of words to consider as features
maxlen = 200  # Cut texts after this number of words

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(train_data)
x_train = tokenizer.texts_to_sequences(train_data)
x_test = tokenizer.texts_to_sequences(test_data)

# Pad sequences to ensure uniform length
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)

# Reverse word index
reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}

# Convert sequences back to text
x_train_text = [' '.join([reverse_word_index.get(i, '?') for i in sequence]) for sequence in x_train]
x_test_text = [' '.join([reverse_word_index.get(i, '?') for i in sequence]) for sequence in x_test]

# Function to extract movie-related aspects/topics using LDA
def extract_aspects(text_data, num_topics=5):
    stop_words = set(stopwords.words('english'))
    punctuation_table = str.maketrans('', '', string.punctuation)
    all_filtered_texts = []

    for text in text_data:
        # Remove punctuation
        text = text.translate(punctuation_table)

        # Tokenize the text
        tokens = word_tokenize(text)

        # Lowercase
        tokens = [word.lower() for word in tokens]

        # Filter out stop words and non-nouns/adjectives
        tokens = [word for word, tag in pos_tag(tokens) if word not in stop_words and (tag.startswith('NN') or tag.startswith('JJ'))]

        # Join the tokens back into a string
        filtered_text = ' '.join(tokens)

        if filtered_text:
            all_filtered_texts.append(filtered_text)

    # Create a Bag of Words representation for all texts
    vectorizer = CountVectorizer(max_features=10000)
    X_train_bow = vectorizer.fit_transform(all_filtered_texts)

    # Apply LDA
    lda_model = gensim.models.LdaModel(corpus=gensim.matutils.Sparse2Corpus(X_train_bow, documents_columns=False),
                                       id2word=dict((v, k) for k, v in vectorizer.vocabulary_.items()),
                                       num_topics=num_topics,
                                       passes=10)

    aspect_keywords = []
    for topic in lda_model.print_topics(num_topics=num_topics, num_words=10):
        aspect_keywords.extend([word for word, _ in lda_model.show_topic(topic[0], topn=10) if word not in stop_words])

    # Count the frequency of each aspect keyword
    aspect_counter = Counter(aspect_keywords)

    # Select the most common movie-related aspects
    most_common_aspects = aspect_counter.most_common(10)

    return [aspect[0] for aspect in most_common_aspects]

# Extract movie-related aspects from the dataset
aspect_keywords = extract_aspects(x_train_text)

# Print the most common movie-related aspects
print("Most Common Movie-Related Aspects:")
for i, aspect in enumerate(aspect_keywords):
    print(f"Aspect {i + 1}: {aspect}")

# Standard Sentiment Analysis Model
def build_standard_model():
    model = Sequential()
    model.add(Embedding(max_features, 128, input_length=maxlen))
    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    return model

# Aspect-Based Sentiment Analysis Model with Aspects
def build_aspect_model_with_aspects_and_regularization(aspect_keywords, l2_lambda=0.01):
    # Input layers
    input_layer = Input(shape=(maxlen,))
    aspect_input_layers = [Input(shape=(1,)) for _ in range(len(aspect_keywords))]

    # Embedding layer
    embedding_layer = Embedding(max_features, 128)(input_layer)
    aspect_embedding_layers = [Embedding(len(reverse_word_index)+1, 128)(aspect_input) for aspect_input in aspect_input_layers]

    # LSTM layer
    lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)

    # Attention mechanism
    query_value_attention_seq = Attention()([lstm_layer, lstm_layer])

    # Flatten the attention output
    attention_flatten = Flatten()(query_value_attention_seq)

    # Dense layer to align dimensions
    dense_attention = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda))(attention_flatten)

    # Concatenate aspect embeddings
    concatenated_aspect_embeddings = Concatenate(axis=1)(aspect_embedding_layers)

    # Concatenate attention output with aspect embeddings
    concatenated_features = Concatenate(axis=1)([tf.expand_dims(dense_attention, axis=1), concatenated_aspect_embeddings])

    # Flatten the concatenated features
    concatenated_flatten = Flatten()(concatenated_features)

    # Dense layer
    dense_layer = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda))(concatenated_flatten)

    # Output layer
    output_layer = Dense(1, activation='sigmoid')(dense_layer)

    model = Model(inputs=[input_layer] + aspect_input_layers, outputs=output_layer)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model


# Prepare aspect input data for aspect-based model
x_train = np.array(x_train)
x_test = np.array(x_test)
train_labels = np.array(train_labels)
test_labels = np.array(test_labels)

batch_size = 32
x_train_aspect_input = []
x_test_aspect_input = []
for i in range(len(aspect_keywords)):
    aspect_index = {word: idx + 1 for idx, word in enumerate(aspect_keywords[i])}
    x_train_aspect_input.append(np.array([[aspect_index.get(reverse_word_index.get(word, 0), 0)] for word in x_train_text]))
    x_test_aspect_input.append(np.array([[aspect_index.get(reverse_word_index.get(word, 0), 0)] for word in x_test_text]))

# Train and evaluate the standard sentiment analysis model
standard_model = build_standard_model()
standard_model_history = standard_model.fit(x_train, train_labels,
                                            validation_split=0.2,
                                            batch_size=batch_size,
                                            epochs=5)

standard_model_score, standard_model_acc = standard_model.evaluate(x_test, test_labels, batch_size=batch_size)
print('Standard Sentiment Analysis Model - Test score:', standard_model_score)
print('Standard Sentiment Analysis Model - Test accuracy:', standard_model_acc)

# Train and evaluate the aspect-based sentiment analysis model with aspects
aspect_model_with_aspects = build_aspect_model_with_aspects_and_regularization(aspect_keywords)
aspect_model_with_aspects_history = aspect_model_with_aspects.fit([x_train] + x_train_aspect_input, train_labels,
                                                                  batch_size=batch_size,
                                                                  epochs=5,
                                                                  validation_split=0.2)

aspect_model_with_aspects_score, aspect_model_with_aspects_acc = aspect_model_with_aspects.evaluate([x_test] + x_test_aspect_input, test_labels, batch_size=batch_size)
print('Aspect-Based Sentiment Analysis Model with Aspects - Test score:', aspect_model_with_aspects_score)
print('Aspect-Based Sentiment Analysis Model with Aspects - Test accuracy:', aspect_model_with_aspects_acc)

# Plotting the training and validation accuracy and loss for the standard model
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(standard_model_history.history['accuracy'], label='train_accuracy')
plt.plot(standard_model_history.history['val_accuracy'], label='val_accuracy')
plt.title('Standard Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(standard_model_history.history['loss'], label='train_loss')
plt.plot(standard_model_history.history['val_loss'], label='val_loss')
plt.title('Standard Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Plotting the training and validation accuracy and loss for the aspect-based model with aspects
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(aspect_model_with_aspects_history.history['accuracy'], label='train_accuracy')
plt.plot(aspect_model_with_aspects_history.history['val_accuracy'], label='val_accuracy')
plt.title('Aspect-Based Model with Aspects Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(aspect_model_with_aspects_history.history['loss'], label='train_loss')
plt.plot(aspect_model_with_aspects_history.history['val_loss'], label='val_loss')
plt.title('Aspect-Based Model with Aspects Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()